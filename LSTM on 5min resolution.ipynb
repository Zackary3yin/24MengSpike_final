{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64599e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, Subset, random_split\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "import pywt\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c9a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGUtil:\n",
    "    @staticmethod\n",
    "    def load_data(file_path, column_names=['spike hz']):\n",
    "        \"\"\"Load multi-channel EEG data from a CSV file and normalize it.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path: Path to the CSV file.\n",
    "        - column_names: List of EEG signal column names to load.\n",
    "\n",
    "        Returns:\n",
    "        - signal: 2D numpy array (channels × time), normalized.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        available_columns = [col for col in column_names if col in data.columns]\n",
    "        if not available_columns:\n",
    "            raise ValueError(f\"None of the specified columns {column_names} exist in {file_path}\")\n",
    "\n",
    "        signal = data[available_columns].values.T  # (channels, time)\n",
    "\n",
    "        mean = np.mean(signal, axis=1, keepdims=True)\n",
    "        std = np.std(signal, axis=1, keepdims=True) \n",
    "        std[std == 0] = 1e-8  \n",
    "        \n",
    "        return signal\n",
    "\n",
    "    @staticmethod\n",
    "    def padding(signal, target_length, mode=\"constant\", constant_value=0):\n",
    "        \"\"\"Pad or truncate multi-channel signal to the target length.\"\"\"\n",
    "        signal = np.array(signal)  # Ensure it's a NumPy array\n",
    "        channels, current_length = signal.shape\n",
    "        \n",
    "        if current_length >= target_length:\n",
    "            return signal[:, :target_length]  # Truncate\n",
    "\n",
    "        padding_size = target_length - current_length\n",
    "        if mode == \"constant\":\n",
    "            pad_values = np.full((channels, padding_size), constant_value)\n",
    "        elif mode == \"reflect\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='reflect')[:, -padding_size:]\n",
    "        elif mode == \"cyclic\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='wrap')[:, -padding_size:]\n",
    "        elif mode == \"edge\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='edge')[:, -padding_size:]\n",
    "        elif mode == \"random\":\n",
    "            pad_values = np.random.uniform(low=np.min(signal), high=np.max(signal), size=(channels, padding_size))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported padding mode: {mode}\")\n",
    "\n",
    "        return np.hstack((signal, pad_values))  # Concatenate along time axis\n",
    "    \n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_folder, outcome_file=None, ssd_file=None,start_time=16,target_length=600, strategy='padding', \n",
    "                 padding_mode=\"constant\", use_labels=False, augment=False,num_good=1,num_bad=1,column_names=['spike hz']):\n",
    "        \n",
    "        \"\"\"\n",
    "        EEG signal datasets with support for data enhancement (random fill). \n",
    "\n",
    "        Parameters: \n",
    "        - data_folder: path of the EEG data folder \n",
    "        - outcome_file: CSV file with patient ID and outcome (optional) \n",
    "        - target_length: indicates the padding length of the target \n",
    "        - strategy: wavelet ('padding', 'wavelet', 'psd') \n",
    "        - padding_mode: indicates the fill mode ('constant', 'reflect',...). \n",
    "        - use_labels: indicates whether to use labels \n",
    "        - augment: Whether data enhancement is enabled (randomly fill different lengths) \n",
    "        \"\"\"\n",
    "\n",
    "        self.use_labels = use_labels\n",
    "        self.data_folder = data_folder\n",
    "        self.target_length = target_length\n",
    "        self.strategy = strategy\n",
    "        self.padding_mode = padding_mode\n",
    "        self.augment = augment \n",
    "        self.column_names = column_names #Feature Chosen\n",
    "        \n",
    "        self.num_good = num_good\n",
    "        self.num_bad = num_bad\n",
    "        \n",
    "        self.start_time=start_time\n",
    "        self.end_time=int((target_length/12)+self.start_time)\n",
    "\n",
    "        self.file_list = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "       \n",
    "        # Read 'rosc sec' start time\n",
    "        ssd_df = pd.read_csv(ssd_file)\n",
    "        # Get `pat_ID`\n",
    "        ssd_df[\"pat_ID\"] = ssd_df[\"fn\"].str.extract(r\"(ICARE_\\d+)\")\n",
    "        #  Take the smallest 'rosc sec' of each 'pat ID' \n",
    "        self.rosc_dict = ssd_df.groupby(\"pat_ID\")[\"rosc_sec\"].min().to_dict()\n",
    "        \n",
    "        # LOAD Labels\n",
    "        self.outcome_dict = {}\n",
    "        if use_labels and outcome_file:\n",
    "            self.outcome_data = pd.read_csv(outcome_file)\n",
    "            self.outcome_dict = self.outcome_data.set_index('pat_ID')['outcome'].to_dict()\n",
    "            self.file_list = [f for f in self.file_list if f.split('.')[0] in self.outcome_dict]\n",
    "        \n",
    "        self.valid_files = []\n",
    "        # Filter EEG data conforming to 16h-68h rules\n",
    "        for f in self.file_list:\n",
    "            pat_id = f.split('.')[0]\n",
    "\n",
    "            if pat_id in self.rosc_dict:\n",
    "                file_path = os.path.join(self.data_folder, f)\n",
    "                signal = EEGUtil.load_data(file_path, column_names=self.column_names)  #Load Multiple Channel\n",
    "                rosc_sec = float(self.rosc_dict[pat_id])  # Ensure `rosc_sec` is a float\n",
    "\n",
    "                # **EEG recording time range**\n",
    "                start_time = rosc_sec\n",
    "                end_time = start_time + signal.shape[1] * 300  # Each point represents 5 minutes (=300 seconds)\n",
    "                # **Skip if the data is completely outside the 16h-68h observation window**\n",
    "                if end_time < self.start_time * 3600 or start_time > self.end_time * 3600:\n",
    "                    #print(f\"❌ Skipping {pat_id}: EEG data is out of 16h-68h range ({start_time/3600:.1f}h - {end_time/3600:.1f}h)\")\n",
    "                    continue  \n",
    "\n",
    "                # **Align to the 16h-68h window**\n",
    "                aligned_signal = self.align_signal(signal, rosc_sec)\n",
    "                self.valid_files.append((f, aligned_signal))\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.valid_files)} valid EEG files (filtered from {len(self.file_list)} total)\")\n",
    "\n",
    "        # **Count Good/Bad Outcome samples**\n",
    "        self.good_outcome_count = sum(1 for f, _ in self.valid_files if self.get_label(f.split('.')[0]) == 1)\n",
    "        self.bad_outcome_count = len(self.valid_files) - self.good_outcome_count\n",
    "\n",
    "        print(f\"Good Outcome: {self.good_outcome_count}, Bad Outcome: {self.bad_outcome_count}\")\n",
    "\n",
    "        # **Data Augmentation: Expanding indices**\n",
    "        self.expanded_indices = []\n",
    "        for idx, (filename, signal) in enumerate(self.valid_files):\n",
    "            patient_id = filename.split('.')[0]\n",
    "            label = self.get_label(patient_id) if self.use_labels else -1\n",
    "\n",
    "            if self.augment:\n",
    "                if self.use_labels:\n",
    "                    # Good Outcome ×10, Bad Outcome ×2\n",
    "                    if label == 1:\n",
    "                        repeat_times = self.num_good\n",
    "                    else:\n",
    "                        repeat_times = self.num_bad\n",
    "                else:\n",
    "                    repeat_times = 1  # Data augmentation for unlabeled data\n",
    "            else:\n",
    "                repeat_times = 1  \n",
    "\n",
    "            for _ in range(repeat_times):\n",
    "                self.expanded_indices.append((idx, label))  # ✅ Store index & label\n",
    "    \n",
    "    def __len__(self):\n",
    "        # print(f\"📏 Dataset __len__: {len(self.expanded_indices)}\")  # Ensure `expanded_indices` length is correct\n",
    "        return len(self.expanded_indices)  # ✅ Must return the number of samples after data augmentation\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx, label = self.expanded_indices[idx]\n",
    "        filename, signal = self.valid_files[original_idx]  # Directly retrieve the **aligned** signal\n",
    "        patient_id = filename.split('.')[0]\n",
    "\n",
    "        # Get label\n",
    "        label = -1\n",
    "        if self.use_labels:\n",
    "            label = self.get_label(patient_id)\n",
    "\n",
    "        # Perform data augmentation (varies each time)\n",
    "        if self.augment:\n",
    "            augmented_signal = self.augment_signal(signal)  # ✅ Apply augmentation directly to the **aligned signal**\n",
    "        else:\n",
    "            augmented_signal = signal  # ✅ Use the aligned signal directly\n",
    "\n",
    "        return torch.tensor(augmented_signal, dtype=torch.float32), label\n",
    "    \n",
    "    def align_signal(self, signal, rosc_sec):\n",
    "        \"\"\" Align EEG data to the 16h-68h observation period \"\"\"\n",
    "\n",
    "        target_length = self.target_length  # Number of `5min` windows for 52 hours (624)\n",
    "        total_signal_length = signal.shape[1]  # Total length of the EEG recording\n",
    "        \n",
    "        rosc_sec = float(rosc_sec)  # ✅ Ensure `rosc_sec` is a float\n",
    "        # print(f\"🔍 Processing patient data: rosc_sec={rosc_sec}, total_signal_length={total_signal_length}\")\n",
    "\n",
    "        # **Calculate the starting position of '16h' in the EEG recording**\n",
    "        start_sec = (self.start_time * 3600) - rosc_sec  \n",
    "        if start_sec < 0:\n",
    "            pad_size = abs(start_sec) / 300  # Calculate the number of windows to pad\n",
    "            start_index = 0  # Start extracting data from the beginning of the EEG recording\n",
    "        else:\n",
    "            pad_size = 0  # No padding needed\n",
    "            start_index = int(start_sec // 300)  # ✅ Convert to integer\n",
    "\n",
    "        # **Calculate the endpoint index for '68h'**\n",
    "        end_index = int(min(start_index + target_length, total_signal_length))  # ✅ Convert to integer\n",
    "\n",
    "        # **Extract EEG data for the 16h-68h observation period**\n",
    "        aligned_signal = signal[:, start_index:end_index]\n",
    "\n",
    "        # **Pre-padding (if `rosc_sec > 16h`)**\n",
    "        if pad_size > 0:\n",
    "            aligned_signal = EEGDataset.pad_signal(aligned_signal, target_length, self.padding_mode, padding_position=\"pre\")\n",
    "\n",
    "        # **Post-padding (if data is less than 52 hours)**\n",
    "        aligned_signal = EEGDataset.pad_signal(aligned_signal, target_length, self.padding_mode, padding_position=\"post\")\n",
    "\n",
    "        # print(f\"✅ Aligned signal length: {len(aligned_signal)}\")\n",
    "        return aligned_signal\n",
    "    \n",
    "    def pad_signal(signal, target_length, mode=\"constant\", constant_value=0, padding_position=\"post\"):\n",
    "        \"\"\" Pad EEG signal to ensure it reaches `target_length`.\n",
    "\n",
    "        Parameters:\n",
    "        - signal: Original EEG signal (numpy array)\n",
    "        - target_length: Target length (52h = 624 `5min` windows)\n",
    "        - mode: Padding mode:\n",
    "            - `constant`: Fill with a fixed value (`constant_value`)\n",
    "            - `reflect`: Mirror padding\n",
    "            - `cyclic`: Cyclic padding\n",
    "            - `edge`: Edge padding\n",
    "            - `random`: Fill with random values between [min, max]\n",
    "        - padding_position: `\"pre\"` (pad at the beginning) or `\"post\"` (pad at the end)\n",
    "\n",
    "        Returns:\n",
    "        - Padded EEG signal (numpy array)\n",
    "        \"\"\"\n",
    "\n",
    "        channels, current_length = signal.shape\n",
    "        \n",
    "        if current_length >= target_length:\n",
    "            return signal[:, :target_length]  # Truncate if already long enough\n",
    "\n",
    "        padding_size = target_length - current_length  # Number of elements to pad\n",
    "\n",
    "        if mode == \"constant\":\n",
    "            pad_values = np.full((channels, padding_size), constant_value)  # Make sure padding shape matches\n",
    "        elif mode == \"reflect\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='reflect')[:, -padding_size:]\n",
    "        elif mode == \"cyclic\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='wrap')[:, -padding_size:]\n",
    "        elif mode == \"edge\":\n",
    "            pad_values = np.pad(signal, ((0, 0), (0, padding_size)), mode='edge')[:, -padding_size:]\n",
    "        elif mode == \"random\":\n",
    "            pad_values = np.random.uniform(low=np.min(signal), high=np.max(signal), size=(channels, padding_size))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported padding mode: {mode}\")\n",
    "\n",
    "        # Ensure proper concatenation along the time axis\n",
    "        if padding_position == \"pre\":\n",
    "            padded_signal = np.hstack((pad_values, signal))  # Pad at the beginning\n",
    "        else:\n",
    "            padded_signal = np.hstack((signal, pad_values))  # Pad at the end\n",
    "\n",
    "        return padded_signal[:, :target_length]  # Ensure exact target length\n",
    "    \n",
    "    def augment_signal(self, signal):\n",
    "        \"\"\" Data augmentation: Shift EEG data within the 16h-68h observation period \"\"\"\n",
    "\n",
    "        target_length = self.target_length  # Number of `5min` windows for 52 hours (624)\n",
    "        channels, current_length = signal.shape  # Current EEG recording length\n",
    "\n",
    "        # **Check if already aligned to 16h-68h before augmentation**\n",
    "        if current_length != target_length:\n",
    "            raise ValueError(f\"Before augmentation, signal length should be {target_length}, but received {current_length}\")\n",
    "\n",
    "        # **Augmentation Strategy 1: Random time shift within ±60min**\n",
    "        max_shift = 12  # `5min` windows, 60 minutes = 12 data points\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)  # Random shift in [-12, 12]\n",
    "\n",
    "        # **Compute new starting index and ensure it remains within bounds**\n",
    "        start_index = max(0, min(current_length - target_length, shift))\n",
    "        end_index = min(start_index + target_length, current_length)\n",
    "\n",
    "        # **Extract EEG data after shifting**\n",
    "        augmented_signal = signal[:, start_index:end_index]\n",
    "\n",
    "        # **Augmentation Strategy 2: Apply padding based on `shift` direction**\n",
    "        if augmented_signal.shape[1] < target_length:\n",
    "            padding_size = target_length - augmented_signal.shape[1]\n",
    "\n",
    "            if shift > 0:\n",
    "                pad_values = self.pad_signal(np.zeros((channels, padding_size)), target_length, self.padding_mode)\n",
    "                augmented_signal = np.hstack((pad_values, augmented_signal))  # **确保形状正确**\n",
    "            elif shift < 0:\n",
    "                pad_values = self.pad_signal(np.zeros((channels, padding_size)), target_length, self.padding_mode)\n",
    "                augmented_signal = np.hstack((augmented_signal, pad_values))\n",
    "            else:\n",
    "                augmented_signal = self.pad_signal(augmented_signal, target_length, self.padding_mode)\n",
    "\n",
    "        return augmented_signal\n",
    "\n",
    "    def get_label(self, patient_id):\n",
    "        \"\"\" Get sample label (1 = Good Outcome, 0 = Bad Outcome) \"\"\"\n",
    "        return 1 if self.outcome_dict.get(patient_id, 'Bad Outcome') == 'Good Outcome' else 0\n",
    "\n",
    "    def compare_data_augmentation(self):\n",
    "        \"\"\" Compare the number of samples before and after data augmentation. \"\"\"\n",
    "        original_count = len(self.valid_files)  # Count only files that meet the 16h condition\n",
    "        augmented_count = len(self.expanded_indices)  # Count the number of augmented samples\n",
    "\n",
    "        print(f\"Data count before augmentation: {original_count}\")\n",
    "        print(f\"Data count after augmentation: {augmented_count}\")\n",
    "        print(f\"Augmentation ratio: {augmented_count / original_count:.2f}x\")\n",
    "\n",
    "        if self.use_labels:\n",
    "            # Count Good Outcome and Bad Outcome samples in original data (filtered_files)\n",
    "            good_outcome_original = sum(1 for f, _ in self.valid_files if self.get_label(f.split('.')[0]) == 1)\n",
    "            bad_outcome_original = original_count - good_outcome_original  # Remaining are Bad Outcome samples\n",
    "\n",
    "            # Count Good Outcome and Bad Outcome samples after augmentation\n",
    "            good_outcome_augmented = sum(\n",
    "                1 for (idx, _) in self.expanded_indices  # ✅ Use only idx, ignore label\n",
    "                if self.get_label(self.valid_files[idx][0].split('.')[0]) == 1\n",
    "            )\n",
    "            bad_outcome_augmented = augmented_count - good_outcome_augmented  # Remaining are Bad Outcome samples\n",
    "\n",
    "            print(f\"Good Outcome before augmentation: {good_outcome_original}, after augmentation: {good_outcome_augmented}\")\n",
    "            print(f\"Bad Outcome before augmentation: {bad_outcome_original}, after augmentation: {bad_outcome_augmented}\")\n",
    "\n",
    "        return original_count, augmented_count\n",
    "    \n",
    "    \n",
    "# AugmentedEEGDataset \n",
    "class AugmentedEEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, eeg_dataset_instance, augment=True, num_good=10, num_bad=2):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.eeg_dataset_instance = eeg_dataset_instance \n",
    "        self.augment = augment\n",
    "        self.num_good = num_good\n",
    "        self.num_bad = num_bad\n",
    "        self.expanded_data = []\n",
    "\n",
    "        for i in range(len(base_dataset)):\n",
    "            signal, label = base_dataset[i]\n",
    "            repeat = num_good if label == 1 else num_bad\n",
    "\n",
    "            for _ in range(repeat):\n",
    "                signal_np = signal.numpy()\n",
    "                if signal_np.ndim == 2:\n",
    "                    augmented_signal = self.eeg_dataset_instance.augment_signal(signal_np)\n",
    "                else:\n",
    "                    augmented_signal = self.eeg_dataset_instance.augment_signal(signal_np[np.newaxis, :])  # **单通道兼容**\n",
    "\n",
    "                self.expanded_data.append((augmented_signal, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expanded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label = self.expanded_data[idx]\n",
    "        return torch.tensor(signal, dtype=torch.float32), label\n",
    "\n",
    "def count_labels(dataset):\n",
    "    labels = [dataset[i][1] for i in range(len(dataset))] \n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b8574",
   "metadata": {},
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameter settings\n",
    "latent_dim = 10  # Dimension of the latent space\n",
    "initial_channels = 128  # Initial number of channels for CNN\n",
    "start_time=16 #Set initial hour\n",
    "seq_length = (68 - start_time) * 12  # Number of `5min` windows for 52 hours (624)\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "alpha = 1e-12  # Adjust KL divergence weight\n",
    "patience = 10  # Define the number of epochs without improvement before stopping training\n",
    "padding_method = 'constant'  # Use constant padding\n",
    "is_augment = False  # Do not use\n",
    "is_VAEtrain_aug=False # Do not use\n",
    "num_good_train=1\n",
    "num_bad_train=1\n",
    "num_good_test=1\n",
    "num_bad_test=1\n",
    "column_names=['ssd','BCI','avgspectent', 'lv_l5']\n",
    "num_eeg_channels=len(column_names)\n",
    "\n",
    "# Define dataset folder paths\n",
    "data_folder = '5min_smoothed_data/'  # Replace with the actual data folder path\n",
    "valid_outcome_data = 'valid_patients_outcome.csv'\n",
    "ssd_file = 'files_art_ssd_fts_predictions.csv'\n",
    "\n",
    "# Create EEG dataset (labels are not used when training VAE)\n",
    "eeg_dataset = EEGDataset(\n",
    "    data_folder='5min_smoothed_data/',\n",
    "    outcome_file='valid_patients_outcome.csv',\n",
    "    ssd_file=ssd_file,\n",
    "    start_time=start_time,\n",
    "    target_length=seq_length,\n",
    "    strategy='padding',\n",
    "    padding_mode=padding_method,\n",
    "    use_labels=True,  # Use labels to determine class-based augmentation\n",
    "    augment=is_augment,  \n",
    "    num_good=1,\n",
    "    num_bad=1,\n",
    "    column_names=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c61b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 244 valid EEG files (filtered from 244 total)\n",
      "Good Outcome: 44, Bad Outcome: 200\n",
      "Fold 1: Accuracy = 77.55%\n",
      "Fold 2: Accuracy = 79.59%\n",
      "Fold 3: Accuracy = 79.59%\n",
      "Fold 4: Accuracy = 87.76%\n",
      "Fold 5: Accuracy = 85.42%\n",
      "Mean Accuracy: 81.98% ± 3.90%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# —— 0. 设备设置 —— \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# —— 1. 超参数与数据集初始化 —— \n",
    "start_time   = 16\n",
    "seq_length   = (68 - start_time) * 12  # 624 个 5min 窗口\n",
    "column_names = ['ssd', 'BCI', 'avgspectent', 'lv_l5']\n",
    "batch_size   = 32\n",
    "epochs       = 100\n",
    "n_splits     = 5\n",
    "learning_rate= 1e-3\n",
    "\n",
    "# 创建 EEGDataset\n",
    "eeg_dataset = EEGDataset(\n",
    "    data_folder='5min_smoothed_data/',\n",
    "    outcome_file='valid_patients_outcome.csv',\n",
    "    ssd_file='files_art_ssd_fts_predictions.csv',\n",
    "    start_time=start_time,\n",
    "    target_length=seq_length,\n",
    "    strategy='padding',\n",
    "    padding_mode='constant',\n",
    "    use_labels=True,\n",
    "    augment=False,\n",
    "    num_good=1,\n",
    "    num_bad=1,\n",
    "    column_names=column_names\n",
    ")\n",
    "\n",
    "# —— 2. 从 Dataset 中提取 X, y, groups —— \n",
    "signals, labels, groups = [], [], []\n",
    "for idx, label in eeg_dataset.expanded_indices:\n",
    "    filename, signal = eeg_dataset.valid_files[idx]\n",
    "    signals.append(signal.T)               # (time_steps, features)\n",
    "    labels.append(label)                   \n",
    "    groups.append(filename.split('.')[0])  # pat_ID\n",
    "\n",
    "X = np.stack(signals)    # shape: (n_samples, time_steps, features)\n",
    "y = np.array(labels)     # shape: (n_samples,)\n",
    "\n",
    "# —— 3. 定义 PyTorch LSTM 模型 —— \n",
    "input_size = X.shape[2]\n",
    "hidden1    = 64\n",
    "hidden2    = 12\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden1, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden1, hidden2, batch_first=True)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc    = nn.Linear(hidden2, 1)\n",
    "        # 注意：不在这里加 Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time_steps, features)\n",
    "        out, _ = self.lstm1(x)\n",
    "        out     = self.drop1(out)\n",
    "        out, _  = self.lstm2(out)\n",
    "        out     = self.drop2(out)\n",
    "        out     = out[:, -1, :]       # 取最后一个时间步，shape (batch, hidden2)\n",
    "        logits  = self.fc(out)        # shape (batch, 1)\n",
    "        return logits.squeeze(1)      # shape (batch,)\n",
    "\n",
    "# —— 4. 分组交叉验证训练和评估 —— \n",
    "kf       = GroupKFold(n_splits=n_splits)\n",
    "loss_fn  = nn.BCEWithLogitsLoss()\n",
    "acc_list = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y, groups=groups), 1):\n",
    "    # 构造训练/测试张量\n",
    "    X_train = torch.tensor(X[train_idx], dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(y[train_idx], dtype=torch.float32, device=device)\n",
    "    X_test  = torch.tensor(X[test_idx],  dtype=torch.float32, device=device)\n",
    "    y_test  = torch.tensor(y[test_idx],  dtype=torch.float32, device=device)\n",
    "\n",
    "    # 初始化模型、优化器\n",
    "    model = LSTMClassifier(input_size).to(device)\n",
    "    opt   = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_train)         # shape (n_train,)\n",
    "        loss   = loss_fn(logits, y_train)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # 测试\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits    = model(X_test)\n",
    "        preds     = (torch.sigmoid(logits) > 0.5).float()\n",
    "        accuracy  = (preds == y_test).float().mean().item()\n",
    "    acc_list.append(accuracy * 100)\n",
    "    print(f\"Fold {fold}: Accuracy = {accuracy*100:.2f}%\")\n",
    "\n",
    "# —— 5. 输出整体结果 —— \n",
    "mean_acc = np.mean(acc_list)\n",
    "std_acc  = np.std(acc_list)\n",
    "print(f\"Mean Accuracy: {mean_acc:.2f}% ± {std_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f84d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 244 valid EEG files (filtered from 244 total)\n",
      "Good Outcome: 44, Bad Outcome: 200\n",
      "Original class balance: Counter({0: 200, 1: 44})\n",
      "----- Fold 1 -----\n",
      "Train balance: Counter({0: 162, 1: 33}) Test balance: Counter({0: 38, 1: 11})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8b/cc9112l96mv2srtjrnzbht8w0000gn/T/ipykernel_9283/423605291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# c) 计算 AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_patient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_patient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mauc_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     if y_type == \"multiclass\" or (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# —— 0. 设备设置 —— \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# —— 1. 超参数与数据集初始化 —— \n",
    "start_time    = 16\n",
    "seq_length    = (68 - start_time) * 12   # 624 个 5min 窗口\n",
    "column_names  = ['ssd', 'BCI', 'avgspectent', 'lv_l5']\n",
    "batch_size    = 8    # 对应原代码 batch_size=8\n",
    "epochs        = 1    # 对应原 model.fit 默认的 1 个 epoch\n",
    "n_splits      = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 创建 EEGDataset\n",
    "eeg_dataset = EEGDataset(\n",
    "    data_folder='5min_smoothed_data/',\n",
    "    outcome_file='valid_patients_outcome.csv',\n",
    "    ssd_file='files_art_ssd_fts_predictions.csv',\n",
    "    start_time=start_time,\n",
    "    target_length=seq_length,\n",
    "    strategy='padding',\n",
    "    padding_mode='constant',\n",
    "    use_labels=True,\n",
    "    augment=False,\n",
    "    num_good=1,\n",
    "    num_bad=1,\n",
    "    column_names=column_names\n",
    ")\n",
    "\n",
    "# —— 2. 从 Dataset 中提取 X, y, groups —— \n",
    "signals, labels, groups = [], [], []\n",
    "for idx, label in eeg_dataset.expanded_indices:\n",
    "    fname, signal = eeg_dataset.valid_files[idx]\n",
    "    signals.append(signal.T)               # (time_steps, features)\n",
    "    labels.append(label)\n",
    "    groups.append(fname.split('.')[0])     # pat_ID\n",
    "\n",
    "X = np.stack(signals)    # shape: (n_samples, time_steps, features)\n",
    "y = np.array(labels)     # shape: (n_samples,)\n",
    "\n",
    "# 标签沿时间步复制，匹配模型输出维度 (n_samples, seq_length)\n",
    "y_seq = np.tile(y[:, None], (1, seq_length))\n",
    "\n",
    "# —— 3. 定义 PyTorch LSTM 模型 —— \n",
    "input_size = X.shape[2]\n",
    "hidden1, hidden2 = 64, 12\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, out_steps):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden1, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden1, hidden2, batch_first=True)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc    = nn.Linear(hidden2, out_steps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out     = self.drop1(out)\n",
    "        out, _  = self.lstm2(out)\n",
    "        out     = self.drop2(out)\n",
    "        out     = out[:, -1, :]\n",
    "        logits  = self.fc(out)      # (batch, seq_length)\n",
    "        return logits               # 不再 sigmoid   # (batch, seq_length)\n",
    "\n",
    "# —— 4. 分组交叉验证训练和评估 —— \n",
    "kf            = GroupKFold(n_splits=n_splits)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "cvscores      = []\n",
    "auc_scores    = []\n",
    "prediction_arr = np.array([])\n",
    "\n",
    "print(\"Original class balance:\", Counter(y))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y, groups=groups), 1):\n",
    "    print(f\"----- Fold {fold} -----\")\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    print(\"Train balance:\", Counter(y_train), \"Test balance:\", Counter(y_test))\n",
    "    \n",
    "    # 准备训练/测试数据\n",
    "    X_tr = torch.tensor(X[train_idx], dtype=torch.float32, device=device)\n",
    "    y_tr = torch.tensor(y_seq[train_idx], dtype=torch.float32, device=device)\n",
    "    X_te = torch.tensor(X[test_idx],  dtype=torch.float32, device=device)\n",
    "    y_te = torch.tensor(y_seq[test_idx], dtype=torch.float32, device=device)\n",
    "    \n",
    "    train_ds     = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 模型、优化器初始化\n",
    "    model     = LSTMClassifier(input_size, seq_length).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练（1 个 epoch，对应原 .fit 的默认行为）\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(xb)\n",
    "        loss = criterion(outs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 测试及评估\n",
    "    # —— 在测试及评估里替换 AUC 计算 —— \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_te)                                 # (n_test, seq_length)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()         # (n_test, seq_length)\n",
    "\n",
    "    # 1) 窗口级 Accuracy（保持原样）\n",
    "    acc = accuracy_score(\n",
    "        y_seq[test_idx].flatten(),\n",
    "        (probs > 0.5).astype(int).flatten()\n",
    "    )\n",
    "    cvscores.append(acc * 100)\n",
    "\n",
    "    # 2) 患者级 AUC\n",
    "   # a) 按时间步平均每个病人的预测概率，NaN 替为 0.5\n",
    "    preds_patient = np.nan_to_num(preds_patient, nan=0.5)\n",
    "    labels_patient = y[test_idx]                    # (n_test,)\n",
    "\n",
    "    # c) 计算 AUC\n",
    "    auc = roc_auc_score(labels_patient, preds_patient)\n",
    "    auc_scores.append(auc * 100)\n",
    "\n",
    "    print(f\"Fold {fold}: Accuracy = {acc*100:.2f}%, AUC = {auc*100:.2f}%\")\n",
    "# —— 5. 输出整体结果 —— \n",
    "print(f\"Mean Accuracy: {np.mean(cvscores):.2f}% ± {np.std(cvscores):.2f}%\")\n",
    "print(f\"Mean AUC:      {np.mean(auc_scores):.2f}% ± {np.std(auc_scores):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8b4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
